# -*- coding: utf-8 -*-
"""Untitled3.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1DdhBwHHnJ6xvhBsJmNjzKtCQ2ZviDr3a
"""

import ast

import pandas as pd
import numpy as np

import tensorflow as tf

from tensorflow import keras
from tensorflow.keras import layers

import matplotlib.pyplot as plt
from rdkit import Chem, RDLogger
from rdkit.Chem import BondType
from rdkit.Chem.Draw import MolsToGridImage
import random
import math
import os
from tensorflow.keras import backend as K
from sklearn.preprocessing import normalize

RDLogger.DisableLog("rdApp.*")
import re

df = pd.read_excel('/home/C00521897/Fall 22/New_Monomer_generation/Data/smiles.xlsx')
df.head()



SMILE_CHARSET = '["C", "B", "F", "I", "H", "O", "N", "S", "P", "Cl", "Br","Si"]'

bond_mapping = {"SINGLE": 0, "DOUBLE": 1, "TRIPLE": 2, "AROMATIC": 3}
bond_mapping.update(
    {0: BondType.SINGLE, 1: BondType.DOUBLE, 2: BondType.TRIPLE, 3: BondType.AROMATIC}
)
SMILE_CHARSET = ast.literal_eval(SMILE_CHARSET)
print(SMILE_CHARSET)
MAX_MOLSIZE = max(df["SMILES"].str.len())
SMILE_to_index = dict((c, i) for i, c in enumerate(SMILE_CHARSET))
index_to_SMILE = dict((i, c) for i, c in enumerate(SMILE_CHARSET))
atom_mapping = dict(SMILE_to_index)
print(atom_mapping)
atom_mapping.update(index_to_SMILE)
print(atom_mapping)

BATCH_SIZE = 16
EPOCHS = 10

VAE_LR = 5e-4
NUM_ATOMS = 160  # Maximum number of atoms

ATOM_DIM = len(SMILE_CHARSET)  # Number of atom types
BOND_DIM = 1  # 5  # Number of bond types
LATENT_DIM = 160#512  # 435 #600  # Size of the latent space

df_smiles = []
df_smiles_greater_2=[]
print(len(df['SMILES']))
for idx in range(len(df['SMILES'])):
    smiles = df["SMILES"][idx].split(',')
    if len(smiles) == 2:
        df_smiles.append(smiles)
    elif len(smiles)>2:
        df_smiles_greater_2.append(smiles)
        
df_smiles_greater_2=np.unique(df_smiles_greater_2)

def smiles_to_graph_d(smiles):
    adjecenies_array = []
    features_array = []
    for i in range(2):
        molecule = Chem.MolFromSmiles(smiles[i])
        adjacency = np.zeros((NUM_ATOMS, NUM_ATOMS), 'float32')
        features = np.zeros((NUM_ATOMS, ATOM_DIM), 'float32')
        for atom in molecule.GetAtoms():
            i = atom.GetIdx()
            atom_type = atom_mapping[atom.GetSymbol()]
            features[i] = np.eye(ATOM_DIM)[atom_type]
            for neighbor in atom.GetNeighbors():
                j = neighbor.GetIdx()
                
                bond = molecule.GetBondBetweenAtoms(i, j)
                bond_type_idx = bond_mapping[bond.GetBondType().name]
                # adjacency[[i,j],[j,i]] = bond_type_idx+1
                adjacency[i, j] = bond_type_idx + 1
        features[np.where(np.sum(features, axis=1) == 0)[0], -1] = 1
        # adjacency[-1,np.sum(adjacency, axis=0)== 0] = 1
        adjecenies_array.append(adjacency)
        features_array.append(features)
    return adjecenies_array[0], features_array[0], adjecenies_array[1], features_array[1]


def formMol(adjacency, features):
    molecule = Chem.RWMol()
    keep_idx = np.where(
        (np.argmax(features, axis=1) != ATOM_DIM - 1))[0]
    features = features[keep_idx]
    adjacency = adjacency[keep_idx, :][:, keep_idx]
    for atom_type_idx in np.argmax(features, axis=1):
        atom = Chem.Atom(atom_mapping[atom_type_idx])
        _ = molecule.AddAtom(atom)

    atoms_i, atoms_j = np.where(np.triu(adjacency) > 0)
    for atom_i, atom_j in zip(atoms_i, atoms_j):
        if atom_i == atom_j:
            continue
        bond_idx = adjacency[atom_i, atom_j] - 1
        if bond_idx >3 or bond_idx<0:
            bond_idx = 0
          
        bond_type = bond_mapping[bond_idx]

        molecule.AddBond(int(atom_i), int(atom_j), bond_type)
    flag = Chem.SanitizeMol(molecule, catchErrors=True)
    if flag != Chem.SanitizeFlags.SANITIZE_NONE:
        return None
    return molecule


def graph_to_molecule2_d(graphs):
    adjecenies_array_0, features_array_0, adjecenies_array_1, features_array_1 = graphs
    all_mols = []
    for i in range(len(adjecenies_array_0)):
        adjacency_0, features_0, adjacency_1, features_1 = adjecenies_array_0[i], features_array_0[i], \
                                                           adjecenies_array_1[i], features_array_1[i]
        mols = [formMol(adjacency_0, features_0), formMol(adjacency_1, features_1)]
        all_mols.append(mols)
    return all_mols

s = df_smiles[77]  # ["C=CC(Cl)=O","NCC(=O)O"]
print(s)
a0, f0, a1, f1 = smiles_to_graph_d(s)
graph = [[a0], [f0], [a1], [f1]]
mols = graph_to_molecule2_d(graph)
print(Chem.MolToSmiles(mols[0][0]), Chem.MolToSmiles(mols[0][1]))

adjacenices_array = []
features_array = []

adjacency_0, features_0, adjacency_1, features_1 = [], [], [], []

for idx in range(len(df_smiles)):
    if idx == 78:
        continue
    smiles = df_smiles[idx]
    adj_0, feat_0, adj_1, feat_1 = smiles_to_graph_d(smiles)
    adjacency_0.append(adj_0)
    features_0.append(feat_0)
    adjacency_1.append(adj_1)
    features_1.append(feat_1)
graph = [adjacency_0, features_0, adjacency_1, features_1]

mols_all = graph_to_molecule2_d(graph)

adjacency_0_tensor = np.array(adjacency_0, dtype='float32')
adjacency_1_tensor = np.array(adjacency_1, dtype='float32')
feature_0_tensor = np.array(features_0, dtype='float32')
feature_1_tensor = np.array(features_1, dtype='float32')

EPOXY_GROUP = 100
IMINE_GROUP = 101
VINYL_GROUP = 102
ACRYLATE_GROUP = 103
THIOL_GROUP = 104

def hasEpoxyGroup(smile):
    mol = Chem.MolFromSmiles(smile)
    substructure = Chem.MolFromSmarts('C1OC1')
    matches = []
    if mol.HasSubstructMatch(substructure):
        matches = mol.GetSubstructMatches(substructure)
    else:
        return (False, 0)
    return (len(matches) >= 2, len(matches), matches, False)


def has_imine(smiles):
    imine_pattern_1 = Chem.MolFromSmarts('NC')
    imine_pattern_2 = Chem.MolFromSmarts('Nc')
    capital_C = False
    mol = Chem.MolFromSmiles(smiles)
    matches = []
    if mol.HasSubstructMatch(imine_pattern_1):
        matches = mol.GetSubstructMatches(imine_pattern_1)
        capital_C = True
    elif mol.HasSubstructMatch(imine_pattern_2):
        matches = mol.GetSubstructMatches(imine_pattern_2)
        capital_C = False
    else:
        return (False, 0)
    return (len(matches) >= 2, len(matches), matches, capital_C)


def has_vinyl_group(smiles):
    vinyl_pattern = Chem.MolFromSmarts('C=C')
    mol = Chem.MolFromSmiles(smiles)

    if mol is not None and vinyl_pattern is not None:
        matches = mol.GetSubstructMatches(vinyl_pattern)
        return (len(matches) >= 2, len(matches), matches, False)
    else:
        return (False, 0)
    
def has_thiol_group(smiles):
    mol = Chem.MolFromSmiles(smiles)
    if mol:
        thiol_substructure = Chem.MolFromSmiles('CCS')
        matches = mol.GetSubstructMatches(thiol_substructure)
        return (len(matches) >= 2, len(matches), matches)
    else:
        return (False,0,0)
def has_acrylate_group(smiles):
    mol = Chem.MolFromSmiles(smiles)
    
    if mol:
        acrylate_substructure = Chem.MolFromSmiles('C=C(C=O)')
        matches = mol.GetSubstructMatches(acrylate_substructure)
        return (len(matches) >= 2, len(matches), matches)
    else:
        return (False,0,0)


def checkFunctionalGroups(smile):
    result_epoxy = hasEpoxyGroup(smile)
    result_imine = has_imine(smile)
    result_vinyl = has_vinyl_group(smile)
    result_thiol = has_thiol_group(smile)
    result_acrylates = has_acrylate_group(smile)
    
    if result_epoxy[0]:
        return (True, result_epoxy[1], result_epoxy[2], EPOXY_GROUP, False)
    elif result_imine[0]:
        return (True, result_imine[1], result_imine[2], IMINE_GROUP, result_imine[3])
    elif result_thiol[0]:
        return (True, result_thiol[1], result_thiol[2], THIOL_GROUP, False)
    elif result_acrylates[0]:
        return (True, result_acrylates[1], result_acrylates[2], ACRYLATE_GROUP, False)
    elif result_vinyl[0]:
        return (True, result_vinyl[1], result_vinyl[2], VINYL_GROUP, False)
    else:
        return (False, 0, 0,0,0)
def checkFunctionalGroups_1(smile, array_data):
    result_epoxy = hasEpoxyGroup(smile)
    result_imine = has_imine(smile)
    result_vinyl = has_vinyl_group(smile)
    result_thiol = has_thiol_group(smile)
    result_acrylates = has_acrylate_group(smile)
    
    if result_epoxy[0]:
        array_data=indexize(result_epoxy[2],array_data, 100)
    if result_imine[0]:  
        array_data=indexize(result_imine[2],array_data, 101)
    if result_thiol[0]:
        array_data=indexize(result_thiol[2],array_data, 102)
    if result_acrylates[0]:
        array_data=indexize(result_acrylates[2],array_data, 103)
    if result_vinyl[0]:
        array_data=indexize(result_vinyl[2],array_data, 104)
    return array_data
    
def indexize(indices,array_data,value):
    for i in range(len(indices)):
        idxs=indices[i]
        for j in range(len(idxs)):
            array_data[0,idxs[j]]=value
        #for j in range(len(idxs)-1):
         #   array_data[idxs[j],idxs[j+1]]=value
          #  array_data[idxs[j+1],idxs[j]]=value
    return array_data
            
        
def make_condition(array_data, type_cond, start_index, condition_indicies, smile, isCapital_C):
    if type_cond == EPOXY_GROUP:
        i1, i2, i3 = condition_indicies
        array_data[0][i1] = 100
        array_data[0][i2] = 100
        array_data[0][i3] = 100
    if type_cond == IMINE_GROUP:
        idx1, idx2 = condition_indicies
        array_data[0][idx1] = 101  # char_to_index['N']
        if isCapital_C:
            array_data[0][idx2] = 101  # char_to_index['C']
        else:
            array_data[0][idx2] = 101  # char_to_index['c']
            
    if type_cond == VINYL_GROUP:
        idx1, idx2 = condition_indicies
        array_data[0][idx1] = 102  
        array_data[0][idx2 ] = 102 
    if type_cond == THIOL_GROUP:
        idx1, idx2, idx3 = condition_indicies
        array_data[0][idx1] = 103  
        array_data[0][idx2 ] = 103 
        array_data[0][idx3 ] = 103 
    if type_cond == ACRYLATE_GROUP:
        idx1, idx2, idx3, idx4 = condition_indicies
        array_data[0][idx1] = 104  
        array_data[0][idx2 ] = 104 
        array_data[0][idx3] = 104  
        array_data[0][idx4 ] = 104
    return array_data
            
        
condition_1 = []
condition_2 = []

def get_imine_condition(sample):
    imine_features_cond_1 = np.zeros((1, NUM_ATOMS), 'float32')
    imine_features_cond_2 = np.zeros((1, NUM_ATOMS), 'float32')
    smiles = sample
    # smiles="CC(C)(c2ccc(OCC1CO1)cc2)c4ccc(OCC3CO3)cc4,CC(C)(c3ccc2OCN(c1ccccc1)Cc2c3)c6ccc5OCN(c4ccccc4)Cc5c6".split(",")
    sm1, sm2 = smiles[0], smiles[1]
    result, result1 = checkFunctionalGroups(sm1), checkFunctionalGroups(sm2)
    imine_count=0
    if result[0] and result1[0]:
        imine_count =np.log10(result[1]+result1[1])# 1#
        
    if result[0]:
        for j in range(result[1]):
            start_index, end_index = min(result[2][j]), max(result[2][j])
            imine_features_cond_1 = make_condition(imine_features_cond_1, result[3], start_index, result[2][j], sm1,
                                                   result[4])

    if result1[0]:
        for i in range(result1[1]):
            start_index, end_index = min(result1[2][i]), max(result1[2][i])
            imine_features_cond_2 = make_condition(imine_features_cond_2, result1[3], start_index, result1[2][i], sm2,
                                                   result1[4])
    return imine_count, imine_features_cond_1, imine_features_cond_2

def get_imine_condition_1(sample):
    imine_features_cond_1 = np.zeros((1, NUM_ATOMS), 'float32')
    imine_features_cond_2 = np.zeros((1, NUM_ATOMS), 'float32')
    smiles = sample
    # smiles="CC(C)(c2ccc(OCC1CO1)cc2)c4ccc(OCC3CO3)cc4,CC(C)(c3ccc2OCN(c1ccccc1)Cc2c3)c6ccc5OCN(c4ccccc4)Cc5c6".split(",")
    sm1, sm2 = smiles[0], smiles[1]
    imine_features_cond_1, imine_features_cond_2 = checkFunctionalGroups_1(sm1,imine_features_cond_1), checkFunctionalGroups_1(sm2,imine_features_cond_2)
    imine_count=0
    return imine_count, imine_features_cond_1, imine_features_cond_2

imine_list=[]
for i in range(len(df_smiles)):
    if i == 78:
        continue
    val, cond1, cond2 = get_imine_condition_1(df_smiles[i])
    condition_1.append(cond1)
    condition_2.append(cond2)
    array_160_1 = np.zeros((NUM_ATOMS, 1))
    imine_list.append(array_160_1+val)

condition_1_array = np.array(condition_1, 'float32')
condition_2_array = np.array(condition_2, 'float32')
imine_features_array=np.array(imine_list,'float32')

print(get_imine_condition(
    ['CC(C)(c2ccc(OCC1CO1)cc2)c4ccc(OCC3OC3)cc4', 'CC(C)(c3ccc2OCN(c1ccccc1)Cc2c3)c6ccc5OCN(c4ccccc4)Cc5c6']))

class RelationalGraphConvLayer(keras.layers.Layer):
    def __init__(self, units=128, activation='relu', use_bias=False, kernel_initializer="glorot_uniform",
                 bias_initializer="zeros",
                 kernel_regularizer=None,
                 bias_regularizer=None, **kwargs):
        super().__init__(**kwargs)
        self.units = units
        self.activation = keras.activations.get(activation)
        self.use_bias = use_bias
        self.kernel_initializer = keras.initializers.get(kernel_initializer)
        self.bias_initializer = keras.initializers.get(bias_initializer)
        self.kernel_regularizer = keras.regularizers.get(kernel_regularizer)
        self.bias_regularizer = keras.regularizers.get(bias_regularizer)

    def build(self, input_shape):
        bond_dim = input_shape[0][1]
        atom_dim = input_shape[1][2]
        atom_dim = 172 
        self.kernel = self.add_weight(shape=(atom_dim, self.units),
                                      initializer=self.kernel_initializer,
                                      regularizer=self.kernel_regularizer,
                                      trainable=True,
                                      name="W",
                                      dtype=tf.float32,
                                      )

        if self.use_bias:
            self.bias = self.add_weight(shape=(1, self.units),
                                        initializer=self.bias_initializer,
                                        regularizer=self.bias_regularizer,
                                        trainable=True,
                                        name="b",
                                        dtype=tf.float32)
            self.built = True

    def call(self, inputs, training=False):
        adjacency_0, features_0, adjacency_1, features_1, imine_feature, conditional_feature_1, conditional_feature_2 = inputs

        x1 = keras.layers.Concatenate(axis=-1)([adjacency_0, features_0])  # tf.matmul(adjacency_0,features_0)
        conditional_features_transposed = tf.transpose(conditional_feature_1, (0, 2, 1))
        x=x1 = keras.layers.Concatenate(axis=-1)([x1,conditional_features_transposed]) #tf.add(x1,conditional_features_transposed)#
        #x=tf.matmul(x1,self.kernel)

        x2 = keras.layers.Concatenate(axis=-1)([adjacency_1, features_1])  # tf.matmul(adjacency_0,features_0)
        conditional_features_transposed_2 = tf.transpose(conditional_feature_2, (0, 2, 1))
        y=x2 =  keras.layers.Concatenate(axis=-1)([x2,conditional_features_transposed_2])#tf.add(x2,conditional_features_transposed_2)#
        #y= tf.matmul(x2, self.kernel)
        
        z=keras.layers.Concatenate(axis=-1)([x,y,imine_feature])

        if self.use_bias:
            x += self.bias
            y += self.bias
        return x+y, x+y#x+y,x*y#x,y#self.activation(z), self.activation(z)


def get_encoder(gconv_units, latent_dim, adjacency_shape, feature_shape, imine_shape, condition_shape,
                dense_units, dropout_rate):
    adjacency_0 = keras.layers.Input(shape=adjacency_shape)
    features_0 = keras.layers.Input(shape=feature_shape)
    adjacency_1 = keras.layers.Input(shape=adjacency_shape)
    features_1 = keras.layers.Input(shape=feature_shape)
    imine_feature = keras.layers.Input(shape=imine_shape)
    conditional_features_1 = keras.layers.Input(shape=condition_shape)
    conditional_features_2 = keras.layers.Input(shape=condition_shape)

    features_transformed_0 = features_0
    features_transformed_1 = features_1

    for units in gconv_units:
        features_transformed_0, features_transformed_1 = RelationalGraphConvLayer(units)(
            [adjacency_0, features_transformed_0, adjacency_1, features_transformed_1, imine_feature
                , conditional_features_1, conditional_features_2])


    #for units in dense_units:
     #   y=x+y
    #    x = layers.Dense(units, activation='relu')(x)
     #   x = layers.Dropout(dropout_rate)(x)
    #    y = layers.Dense(units, activation='relu')(y)
     #   y = layers.Dropout(dropout_rate)(y)
    x = layers.Dense(512, activation='relu')(features_transformed_0)
    y=layers.Dense(512, activation='relu')(features_transformed_1)
    x= keras.layers.LayerNormalization()(x)
    y= keras.layers.LayerNormalization()(y)    
    #x= x+ keras.layers.Attention(use_scale=True)([x,y])
    #y= y + keras.layers.Attention(use_scale=True)([y,x])
    x= x+ keras.layers.Attention(use_scale=True)([x,x])
    y= y + keras.layers.Attention(use_scale=True)([y,y])
    x = keras.layers.GlobalAveragePooling1D()(x)
    y = keras.layers.GlobalAveragePooling1D()(y)
   
   

    z_mean = layers.Dense(latent_dim, dtype='float32', name='z_mean')(x)
    log_var = layers.Dense(latent_dim, dtype='float32', name='log_var')(x)
    z_mean_1 = layers.Dense(latent_dim, dtype='float32', name='z_mean_1')(y)
    log_var_1 = layers.Dense(latent_dim, dtype='float32', name='log_var_1')(y)
    encoder = keras.Model([adjacency_0, features_0, adjacency_1, features_1,
                           imine_feature, conditional_features_1, conditional_features_2],
                          [z_mean, log_var, z_mean_1, log_var_1], name='encoder')

    return encoder


def get_decoder(dense_units, dropout_rate, latent_dim, adjacency_shape, feature_shape, condition_shape):
    latent_inputs = keras.Input(shape=(latent_dim,))
    latent_inputs_1 = keras.Input(shape=(latent_dim,))

    x = latent_inputs
    y = latent_inputs_1
    #for units in dense_units:
    #    x = keras.layers.Dense(units, activation="tanh")(x)
     #   x = keras.layers.Dropout(dropout_rate)(x)
     #   y = keras.layers.Dense(units, activation="tanh")(y)
     #   y = keras.layers.Dropout(dropout_rate)(y)
    x = layers.Dense(512, activation='relu')(x)
    y=layers.Dense(512, activation='relu')(y)
    x= keras.layers.LayerNormalization()(x)
    y= keras.layers.LayerNormalization()(y)    
    x= x+keras.layers.Attention(use_scale=True)([x,x])
    y= y+keras.layers.Attention(use_scale=True)([y,y])
    
   

    x_0_adjacency = keras.layers.Dense(tf.math.reduce_prod(adjacency_shape))(x)
    x_0_adjacency = keras.layers.Reshape(adjacency_shape)(x_0_adjacency)
    # Symmetrify tensors in the last two dimensions
    x_0_adjacency = (x_0_adjacency + tf.transpose(x_0_adjacency, (0, 2, 1)))

    # Map outputs of previous layer (x) to [continuous] feature tensors (x_features)
    x_0_features = keras.layers.Dense(tf.math.reduce_prod(feature_shape))(x)
    x_0_features = keras.layers.Reshape(feature_shape)(x_0_features)
    x_0_features = keras.layers.Softmax(axis=2)(x_0_features)

    # Map outputs of previous layer (x) to [continuous] adjacency tensors (x_adjacency)
    x_1_adjacency = keras.layers.Dense(tf.math.reduce_prod(adjacency_shape))(y)
    x_1_adjacency = keras.layers.Reshape(adjacency_shape)(x_1_adjacency)
    # Symmetrify tensors in the last two dimensions
    x_1_adjacency = (x_1_adjacency + tf.transpose(x_1_adjacency, (0, 2, 1)))
   

    # Map outputs of previous layer (x) to [continuous] feature tensors (x_features)
    x_1_features = keras.layers.Dense(tf.math.reduce_prod(feature_shape))(y)
    x_1_features = keras.layers.Reshape(feature_shape)(x_1_features)
    x_1_features = keras.layers.Softmax(axis=2)(x_1_features)

    decoder = keras.Model(
        [latent_inputs, latent_inputs_1], outputs=[x_0_adjacency, x_0_features, x_1_adjacency, x_1_features],
        name="decoder"
    )

    return decoder


class Sampling(layers.Layer):
    def call(self, inputs):
        z_mean, z_log_var, z_mean_1, z_log_var_1 = inputs
        batch = tf.shape(z_log_var)[0]
        dim = tf.shape(z_log_var)[1]
        epsilon = tf.keras.backend.random_normal(shape=(batch, dim))
        return z_mean + tf.exp(0.5 * z_log_var) * epsilon, z_mean_1 + tf.exp(0.5 * z_log_var_1) * epsilon


class MoleculeGenerator(keras.Model):
    def __init__(self, encoder, decoder, max_len, **kwargs):
        super().__init__(**kwargs)
        self.encoder = encoder
        self.decoder = decoder
        self.imine_prediction_layer = layers.Dense(1)
        self.max_len = max_len

        self.train_total_loss_tracker = keras.metrics.Mean(name="train_total_loss")
        self.train_loss1_tracker = keras.metrics.Mean(name="train_total_loss1")
        self.train_loss2_tracker = keras.metrics.Mean(name="train_total_loss2")
        self.val_total_loss_tracker = keras.metrics.Mean(name="val_total_loss")

    def _gradient_penalty(self, graph_real, graph_generated,imine_tensor, condition_1, condition_2):
        # Unpack graphs
        adjacency_0_real, features_0_real, adjacency_1_real, features_1_real = graph_real
        adjacency_0_generated, features_0_generated, adjacency_1_generated, features_1_generated = graph_generated

        # Generate interpolated graphs (adjacency_interp and features_interp)
        alpha = tf.random.uniform([32])
        alpha = tf.reshape(alpha, (32, 1, 1))
        adjacency_0_interp = (adjacency_0_real * alpha) + (1 - alpha) * adjacency_0_generated
        adjacency_1_interp = (adjacency_1_real * alpha) + (1 - alpha) * adjacency_1_generated
        alpha = tf.reshape(alpha, (32, 1, 1))
        features_0_interp = (features_0_real * alpha) + (1 - alpha) * features_0_generated
        features_1_interp = (features_1_real * alpha) + (1 - alpha) * features_1_generated

        # Compute the logits of interpolated graphs
        with tf.GradientTape() as tape:
            tape.watch(adjacency_0_interp)
            tape.watch(features_0_interp)
            tape.watch(adjacency_1_interp)
            tape.watch(features_1_interp)
            _, _, _, _, _, _, _, _, logits = self(
                [adjacency_0_interp, features_0_interp, adjacency_1_interp, features_1_interp,imine_tensor, condition_1, condition_2], training=True
            )

        # Compute the gradients with respect to the interpolated graphs
        grads = tape.gradient(logits, [adjacency_0_interp, features_0_interp, adjacency_1_interp, features_1_interp])

        # Compute the gradient penalty
        grads_adjacency_0_penalty = (1 - tf.norm(grads[0], axis=1)) ** 2
        grads_features_0_penalty = (1 - tf.norm(grads[1], axis=2)) ** 2
        grads_adjacency_1_penalty = (1 - tf.norm(grads[2], axis=1)) ** 2
        grads_features_1_penalty = (1 - tf.norm(grads[3], axis=2)) ** 2
        return tf.reduce_mean(
            tf.reduce_mean(grads_adjacency_0_penalty, axis=(-2, -1))
            + tf.reduce_mean(grads_features_0_penalty, axis=(-1))
            + tf.reduce_mean(grads_adjacency_1_penalty, axis=(-2, -1))  # need to check
            + tf.reduce_mean(grads_features_1_penalty, axis=(-1))  # need to check
        )

    def train_step(self, data):
        adjacency_0_tensor, feature_0_tensor, adjacency_1_tensor, feature_1_tensor, imine_tensor, condition_1, condition_2 = \
        data[0]
        graph_real = [adjacency_0_tensor, feature_0_tensor, adjacency_1_tensor, feature_1_tensor]
        # self.batch_size = tf.shape(adjacency_0_tensor)[0]

        with tf.GradientTape() as tape:
            z_mean, z_log_var, z_mean_1, z_log_var_1, gen_0_adjacency, gen_0_features, gen_1_adjacency, gen_1_features, imine_pred = self(
                [adjacency_0_tensor, feature_0_tensor, adjacency_1_tensor, feature_1_tensor, imine_tensor, condition_1,
                 condition_2], training=True)
            graph_generated = [gen_0_adjacency, gen_0_features, gen_1_adjacency, gen_1_features]
            total_loss = self._compute_loss(
                z_log_var, z_mean, z_mean_1, z_log_var_1, graph_real, graph_generated, imine_tensor, imine_pred, condition_1, condition_2
            )
        grads = tape.gradient([total_loss], self.trainable_weights)
        self.optimizer.apply_gradients(zip(grads, self.trainable_weights))

        self.train_total_loss_tracker.update_state(total_loss)
        # self.train_loss1_tracker.update_state(loss1)
        # self.train_loss2_tracker.update_state(loss2)
        return {'loss': self.train_total_loss_tracker.result()}
        # {'loss' : self.train_total_loss_tracker.result(),'loss1' : self.train_loss1_tracker.result(),'loss2' : self.train_loss2_tracker.result()}

    def cosine_similarity_loss(self, y_true, y_pred):
        dot_product = tf.reduce_sum(y_true * y_pred)
        norm_tensor1 = tf.norm(y_true)
        norm_tensor2 = tf.norm(y_pred)
        cosine_similarity = dot_product / (norm_tensor1 * norm_tensor2)
        return cosine_similarity

    def _compute_loss(self, z_log_var, z_mean, z_mean_1, z_log_var_1, graph_real, graph_generated, imine_tensor,
                      imine_pred, condition_1, condition_2):
        adjacency_0_real, features_0_real, adjacency_1_real, features_1_real = graph_real
        adjacency_0_gen, features_0_gen, adjacency_1_gen, features_1_gen = graph_generated

        adjacency_0_loss = tf.reduce_mean(
            tf.keras.losses.mean_squared_error(adjacency_0_real, adjacency_0_gen)
        )
        
        adjacency_1_loss = tf.reduce_mean(
            tf.keras.losses.mean_squared_error(adjacency_1_real, adjacency_1_gen)
        )
        
        
        features_0_loss = tf.reduce_mean(
            tf.reduce_sum(
                keras.losses.categorical_crossentropy(features_0_real, features_0_gen),
                axis=(1),
            )
        )
        
        features_1_loss = tf.reduce_mean(
            tf.reduce_sum(
                keras.losses.categorical_crossentropy(features_1_real, features_1_gen),
                axis=(1),
            )
        )
        
        #weighted_loss = features_0_real * 1.0 * features_0_loss + (features_0_gen - features_0_real) * 1.0 * features_0_loss
        #mean_loss_1 = tf.reduce_mean(weighted_loss)
        
        #weighted_loss = features_1_real * 1.0 * features_1_loss + (features_1_gen - features_1_real) * 1.0 * features_1_loss
        #mean_loss_2 = tf.reduce_mean(weighted_loss)
        kl_loss_0 = -0.5 * tf.reduce_sum(
            1 + z_log_var - tf.square(z_mean) - tf.exp(z_log_var), 1
        )
        kl_loss_0 = tf.reduce_mean(kl_loss_0)
        kl_loss_1 = -0.5 * tf.reduce_sum(
            1 + z_log_var_1 - tf.square(z_mean_1) - tf.exp(z_log_var_1), 1
        )
        kl_loss_1 = tf.reduce_mean(kl_loss_1)

        mse = keras.losses.MeanSquaredError(reduction=tf.keras.losses.Reduction.NONE)
        #property_loss = mse(imine_tensor, imine_pred)
        #graph_loss = self._gradient_penalty(graph_real, graph_generated, imine_tensor, condition_1, condition_2)
        
        return adjacency_0_loss + features_1_loss + kl_loss_0 + adjacency_1_loss + features_0_loss + kl_loss_1 
        

    def inference(self, batch_size, sample):
        #np.random.seed(121)
        #tf.random.set_seed(42)
        ad_0, feat_0, ad_1, feat_1 = smiles_to_graph_d(sample)
        g = [[ad_0], [feat_0], [ad_1], [feat_1]]
        all_mols = []
        imine_list=[]

        # temp1 = round(np.random.uniform(0, 4),3)
        # feat_0 = feat_0 + temp1
        # feat_1 = feat_1 + temp1
        random_imine_value = np.random.uniform(0, 2)  # random.randint(0, 1)
        array_160_1 = np.zeros((NUM_ATOMS, 1))
        imine_list.append(array_160_1+random_imine_value)
        
        _,cond1, cond2 = get_imine_condition_1(sample)
        print(random_imine_value)
        z_mean, log_var, z_mean_1, log_var_1 = model.encoder([np.array([ad_0]), np.array([feat_0]),
                                                              np.array([ad_1]), np.array([feat_1]),
                                                              np.array(imine_list),
                                                              np.array([cond1]), np.array([cond2])])
        z1, z2 = Sampling()([z_mean, log_var, z_mean_1, log_var_1])
        
        min_val=np.mean(z1)
        max_val=np.std(z1)
        
        min_val_1=np.mean(z2)
        max_val_1=np.std(z2)


        for i in range(batch_size):
            z = tf.random.normal((1, LATENT_DIM))
            z_p = tf.random.normal((1, LATENT_DIM))
            #z = np.random.normal(min_val,max_val,(1, LATENT_DIM))
            #z_p = np.random.normal(min_val_1,max_val_1,(1, LATENT_DIM))
            z3 =  z1+z#np.multiply(z1, z) #
            z4 = z2+z_p#np.multiply(z1, z_p) #
            reconstruction_adjacnency_0, recontstruction_features_0, reconstruction_adjacnency_1, recontstruction_features_1 = model.decoder.predict(
                [z3, z4])

            adjacency_0 = tf.linalg.set_diag(reconstruction_adjacnency_0,
                                             tf.zeros(tf.shape(reconstruction_adjacnency_0)[:-1]))
            adjacency_0 = abs(reconstruction_adjacnency_0[0].astype(int))
            features_0 = tf.argmax(recontstruction_features_0, axis=2)
            features_0 = tf.one_hot(features_0, depth=ATOM_DIM, axis=2)

            adjacency_1 = tf.linalg.set_diag(reconstruction_adjacnency_1,
                                             tf.zeros(tf.shape(reconstruction_adjacnency_1)[:-1]))
            features_1 = tf.argmax(recontstruction_features_1, axis=2)
            features_1 = tf.one_hot(features_1, depth=ATOM_DIM, axis=2)
            adjacency_1 = abs(reconstruction_adjacnency_1[0].astype(int))
            # graph2=[[adjacency_0[0].numpy()],[features_0[0].numpy()],[adjacency_1[0].numpy()],[features_1[0].numpy()]]
            graph2 = [[adjacency_0], [features_0[0].numpy()],
                      [adjacency_1], [features_0[0].numpy()]]
            all_mols.append(graph_to_molecule2_d(graph2))
        return all_mols

    def call(self, inputs):
        z_mean, log_var, z_mean_1, log_var_1 = self.encoder(inputs)
        z, z1 = Sampling()([z_mean, log_var, z_mean_1, log_var_1])
        gen_adjacency_0, gen_features_0, gen_adjacency_1, gen_features_1 = self.decoder([z, z1])
        imine_pred = self.imine_prediction_layer(z_mean + z_mean_1)

        return z_mean, log_var, z_mean_1, log_var_1, gen_adjacency_0, gen_features_0, gen_adjacency_1, gen_features_1, imine_pred


vae_optimizer = tf.keras.optimizers.Adam(learning_rate=VAE_LR)

encoder = get_encoder(
    gconv_units=[9],
    adjacency_shape=(NUM_ATOMS, NUM_ATOMS),
    feature_shape=(NUM_ATOMS, ATOM_DIM),
    imine_shape= (NUM_ATOMS,1),
    condition_shape=(1,NUM_ATOMS),
    latent_dim=LATENT_DIM,
    dense_units=[128,256, 512],
    dropout_rate=0.2,
)
decoder = get_decoder(
    dense_units=[128,256, 512, 1024],
    dropout_rate=0.2,
    latent_dim=LATENT_DIM,
    adjacency_shape=(NUM_ATOMS, NUM_ATOMS),
    feature_shape=(NUM_ATOMS, ATOM_DIM),
    condition_shape=(1,NUM_ATOMS)
)

model = MoleculeGenerator(encoder, decoder, 160)

checkpoint_path = "/home/C00521897/Fall 22/New_Monomer_generation/checkpoints1/cp-{epoch:04d}.ckpt"
checkpoint_dir = os.path.dirname(checkpoint_path)
n_batches = len(adjacency_0_tensor) / BATCH_SIZE
n_batches = math.ceil(n_batches)
cp_callback = tf.keras.callbacks.ModelCheckpoint(
    filepath=checkpoint_path,
    verbose=1,
    save_weights_only=True,
    save_freq=5 * n_batches)

stopping_callback = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=15, verbose=1, restore_best_weights=True)
# model.save_weights(checkpoint_path.format(epoch=0))

model.compile(vae_optimizer)
history = model.fit([adjacency_0_tensor, feature_0_tensor, adjacency_1_tensor, feature_1_tensor,
                     imine_features_array, condition_1_array, condition_2_array],
                    epochs=100,callbacks=[stopping_callback])


def write_samples_in_file():
    mols = []
    i=0
    with open('/home/C00521897/Fall 22/New_Monomer_generation/Data/output_1/output_10086.txt', 'a') as the_file:
        # random.seed(121)
        smiles = random.choices(df_smiles, k=25)
        for sample in smiles:
            i = i+1
            text = '\n-----------------'+str(i)+'---------------------------------\n'
            mols = model.inference(5, sample)
            the_file.write(text)
            the_file.write(str(sample))
            the_file.write('\n--------------------------------------------------\n')
            for index, m in enumerate(mols):
                if m[0][0] is not None:
                    smiles = Chem.MolToSmiles(m[0][0])
                else:
                    smiles = "None"
                smiles = smiles + ","
                if m[0][1] is not None:
                    smiles += Chem.MolToSmiles(m[0][1]) + "\n"
                else:
                    smiles += "None\n"
                the_file.write(smiles)


write_samples_in_file()


