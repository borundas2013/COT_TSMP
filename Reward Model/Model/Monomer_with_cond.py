# -*- coding: utf-8 -*-
"""Untitled3.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1DdhBwHHnJ6xvhBsJmNjzKtCQ2ZviDr3a
"""

import ast

import pandas as pd
import numpy as np

import tensorflow as tf

from tensorflow import keras
from tensorflow.keras import layers

import matplotlib.pyplot as plt
from rdkit import Chem, RDLogger
from rdkit.Chem import BondType
from rdkit.Chem.Draw import MolsToGridImage
import random
import math
import os
from tensorflow.keras import backend as K
from sklearn.preprocessing import normalize

RDLogger.DisableLog("rdApp.*")
import re

df = pd.read_excel('/home/C00521897/Fall 22/New_Monomer_generation/Data/smiles.xlsx')
df.head()

SMILE_CHARSET = '["C", "B", "F", "I", "H", "O", "N", "S", "P", "Cl", "Br","Si"]'

bond_mapping = {"SINGLE": 0, "DOUBLE": 1, "TRIPLE": 2, "AROMATIC": 3}
bond_mapping.update(
    {0: BondType.SINGLE, 1: BondType.DOUBLE, 2: BondType.TRIPLE, 3: BondType.AROMATIC}
)
SMILE_CHARSET = ast.literal_eval(SMILE_CHARSET)
print(SMILE_CHARSET)
MAX_MOLSIZE = max(df["SMILES"].str.len())
SMILE_to_index = dict((c, i) for i, c in enumerate(SMILE_CHARSET))
index_to_SMILE = dict((i, c) for i, c in enumerate(SMILE_CHARSET))
atom_mapping = dict(SMILE_to_index)
print(atom_mapping)
atom_mapping.update(index_to_SMILE)
print(atom_mapping)

BATCH_SIZE = 16
EPOCHS = 10

VAE_LR = 5e-4
NUM_ATOMS = 160  # Maximum number of atoms

ATOM_DIM = len(SMILE_CHARSET)  # Number of atom types
BOND_DIM = 1  # 5  # Number of bond types
LATENT_DIM = 160#512  # 435 #600  # Size of the latent space

df_smiles = []
for idx in range(len(df['SMILES'])):
    smiles = df["SMILES"][idx].split(',')
    if len(smiles) == 2:
        df_smiles.append(smiles)


def smiles_to_graph_d(smiles):
    adjecenies_array = []
    features_array = []
    for i in range(2):
        molecule = Chem.MolFromSmiles(smiles[i])
        adjacency = np.zeros((NUM_ATOMS, NUM_ATOMS), 'float32')
        features = np.zeros((NUM_ATOMS, ATOM_DIM), 'float32')
        for atom in molecule.GetAtoms():
            i = atom.GetIdx()
            atom_type = atom_mapping[atom.GetSymbol()]
            features[i] = np.eye(ATOM_DIM)[atom_type]
            for neighbor in atom.GetNeighbors():
                j = neighbor.GetIdx()
                
                bond = molecule.GetBondBetweenAtoms(i, j)
                bond_type_idx = bond_mapping[bond.GetBondType().name]
                # adjacency[[i,j],[j,i]] = bond_type_idx+1
                adjacency[i, j] = bond_type_idx + 1
        features[np.where(np.sum(features, axis=1) == 0)[0], -1] = 1
        # adjacency[-1,np.sum(adjacency, axis=0)== 0] = 1
        adjecenies_array.append(adjacency)
        features_array.append(features)
    return adjecenies_array[0], features_array[0], adjecenies_array[1], features_array[1]


def formMol(adjacency, features):
    molecule = Chem.RWMol()
    keep_idx = np.where(
        (np.argmax(features, axis=1) != ATOM_DIM - 1))[0]
    features = features[keep_idx]
    adjacency = adjacency[keep_idx, :][:, keep_idx]
    for atom_type_idx in np.argmax(features, axis=1):
        atom = Chem.Atom(atom_mapping[atom_type_idx])
        _ = molecule.AddAtom(atom)

    atoms_i, atoms_j = np.where(np.triu(adjacency) > 0)
    for atom_i, atom_j in zip(atoms_i, atoms_j):
        if atom_i == atom_j:
            continue
        bond_idx = adjacency[atom_i, atom_j] - 1
        bond_type = bond_mapping[bond_idx]

        molecule.AddBond(int(atom_i), int(atom_j), bond_type)
    flag = Chem.SanitizeMol(molecule, catchErrors=True)
    if flag != Chem.SanitizeFlags.SANITIZE_NONE:
        return None
    return molecule


def graph_to_molecule2_d(graphs):
    adjecenies_array_0, features_array_0, adjecenies_array_1, features_array_1 = graphs
    all_mols = []
    for i in range(len(adjecenies_array_0)):
        adjacency_0, features_0, adjacency_1, features_1 = adjecenies_array_0[i], features_array_0[i], \
                                                           adjecenies_array_1[i], features_array_1[i]
        mols = [formMol(adjacency_0, features_0), formMol(adjacency_1, features_1)]
        all_mols.append(mols)
    return all_mols


s = df_smiles[77]  # ["C=CC(Cl)=O","NCC(=O)O"]
print(s)
a0, f0, a1, f1 = smiles_to_graph_d(s)
graph = [[a0], [f0], [a1], [f1]]
mols = graph_to_molecule2_d(graph)
print(Chem.MolToSmiles(mols[0][0]), Chem.MolToSmiles(mols[0][1]))

adjacenices_array = []
features_array = []

adjacency_0, features_0, adjacency_1, features_1 = [], [], [], []
df_smiles = []
for idx in range(len(df['SMILES'])):
    smiles = df["SMILES"][idx].split(',')
    smile = smiles[0]
    if len(smiles) == 2:
        df_smiles.append(smiles)

for idx in range(len(df_smiles)):
    if idx == 78:
        continue
    smiles = df_smiles[idx]
    adj_0, feat_0, adj_1, feat_1 = smiles_to_graph_d(smiles)
    adjacency_0.append(adj_0)
    features_0.append(feat_0)
    adjacency_1.append(adj_1)
    features_1.append(feat_1)
graph = [adjacency_0, features_0, adjacency_1, features_1]

mols_all = graph_to_molecule2_d(graph)

# train_df = df.sample(frac=1, random_state=42)
# train_df.reset_index(drop=True,inplace=True)
adjacency_0_tensor = np.array(adjacency_0, dtype='float32')
adjacency_1_tensor = np.array(adjacency_1, dtype='float32')
feature_0_tensor = np.array(features_0, dtype='float32')
feature_1_tensor = np.array(features_1, dtype='float32')

print(len(df_smiles))

EPOXY_GROUP_VALUE = 100
IMINE_GROUP_VALUE = 101
DOUBLE_C_GROUP = 102


# unique_characters_list=[]
# def extract_unique_characters(smile):
#     smile=smile.replace('[Si]','L')
#     regex_pattern = r'([A-Z]|[a-z]|\d|\[[^\]]*\]|[\(\)=, \/]|[\\,\#])'


#     matches = re.findall(regex_pattern, smile)

#     unique_characters = set(matches)
#     for uIc in unique_characters:
#         if uIc not in unique_characters_list:
#             unique_characters_list.append(uIc)


# #text_data = "CnCCccc[Si]Cc[Pi]12Ps"
# #print(extract_unique_characters(text_data))
# df['SMILES'].apply(extract_unique_characters)
# print("Unique characters:", unique_characters_list, len(unique_characters_list))
# #index=unique_characters_list.index("[Si]")
# #unique_characters_list[index]='L'
# char_to_index = dict((c, i+1) for i, c in enumerate(unique_characters_list))
# index_to_char = dict((i+1, c) for i, c in enumerate(unique_characters_list))
# print(char_to_index)
# print(index_to_char)

def hasEpoxyGroup(smile):
    mol = Chem.MolFromSmiles(smile)
    substructure = Chem.MolFromSmarts('COC')
    matches = []
    if mol.HasSubstructMatch(substructure):
        matches = mol.GetSubstructMatches(substructure)
    else:
        return (False, 0)
    return (len(matches) >= 2, len(matches), matches, False)


# print(hasEpoxyGroup("CC(C)(c2ccc(OCC1CO1)cc2)c4ccc(OCC3CO3)cc4"))
def has_imine(smiles):
    imine_pattern_1 = Chem.MolFromSmarts('NC')
    imine_pattern_2 = Chem.MolFromSmarts('Nc')
    capital_C = False
    mol = Chem.MolFromSmiles(smiles)
    matches = []
    if mol.HasSubstructMatch(imine_pattern_1):
        matches = mol.GetSubstructMatches(imine_pattern_1)
        capital_C = True
    elif mol.HasSubstructMatch(imine_pattern_2):
        matches = mol.GetSubstructMatches(imine_pattern_2)
        capital_C = False
    else:
        return (False, 0)
    return (len(matches) >= 2, len(matches), matches, capital_C)


# print(has_imine("CC(C)(c3ccc2OCN(c1ccccc1)Cc2c3)c6ccc5OCN(c4ccccc4)Cc5c6"))
def has_double_bonds(smiles):
    double_bond_pattern = Chem.MolFromSmarts('C=C')
    mol = Chem.MolFromSmiles(smiles)

    if mol is not None and double_bond_pattern is not None:
        matches = mol.GetSubstructMatches(double_bond_pattern)
        return (len(matches) >= 2, len(matches), matches, False)
    else:
        return (False, 0)


def checkFunctionalGroups(smile):
    ep1 = hasEpoxyGroup(smile)
    ep4 = has_imine(smile)
    d1 = has_double_bonds(smile)
    if ep1[0]:
        return (True, ep1[1], ep1[2], EPOXY_GROUP_VALUE, False)
    elif ep4[0]:
        return (True, ep4[1], ep4[2], IMINE_GROUP_VALUE, ep4[3])
    elif d1[0]:
        return (True, d1[1], d1[2], DOUBLE_C_GROUP, False)
    else:
        return (False, 0, 0)


def make_condition(array_data, type_cond, start_index, condition_indicies, smile, isCapital_C):
    if type_cond == EPOXY_GROUP_VALUE:
        i1, i2, i3 = condition_indicies
        array_data[0][i1] = 111
        array_data[0][i2] = 111
        array_data[0][i3] = 111
        # pattern = re.compile(r'(C\d{1,7}CO\d{1,7})|(C\d{1,7}OC\d{1,7})', re.IGNORECASE)
        # matches = [(match.start(), match.end() - 1) for match in pattern.finditer(smile)]
        # if matches:
        #     for start, end in matches:
        #         for i in range(start, end+1):
        #             array_data[0][i]=char_to_index[smile[i]]
    if type_cond == IMINE_GROUP_VALUE:

        # cond="Nc"
        idx1, idx2 = condition_indicies
        array_data[0][idx1] = 112  # char_to_index['N']
        if isCapital_C:
            array_data[0][idx2] = 112  # char_to_index['C']
        else:
            array_data[0][idx2] = 112  # char_to_index['c']

    if type_cond == DOUBLE_C_GROUP:
        idx1, idx2 = condition_indicies
        array_data[0][idx1] = 113  # char_to_index["C"]
        array_data[0][idx2] = 113  # char_to_index["="]
        array_data[0][idx2 + 1] = 113  # char_to_index["C"]
    return array_data


condition_1 = []
condition_2 = []

def get_imine_condition(sample):
    imine_features_cond_1 = np.zeros((1, NUM_ATOMS), 'float32')
    imine_features_cond_2 = np.zeros((1, NUM_ATOMS), 'float32')
    smiles = sample
    # smiles="CC(C)(c2ccc(OCC1CO1)cc2)c4ccc(OCC3CO3)cc4,CC(C)(c3ccc2OCN(c1ccccc1)Cc2c3)c6ccc5OCN(c4ccccc4)Cc5c6".split(",")
    sm1, sm2 = smiles[0], smiles[1]
    result, result1 = checkFunctionalGroups(sm1), checkFunctionalGroups(sm2)
    imine_count=0
    if result[0] and result1[0]:
        imine_count = 1#np.log10(result[1]+result1[1])
        
    if result[0]:
        for j in range(result[1]):
            start_index, end_index = min(result[2][j]), max(result[2][j])
            imine_features_cond_1 = make_condition(imine_features_cond_1, result[3], start_index, result[2][j], sm1,
                                                   result[4])

    if result1[0]:
        for i in range(result1[1]):
            start_index, end_index = min(result1[2][i]), max(result1[2][i])
            imine_features_cond_2 = make_condition(imine_features_cond_2, result1[3], start_index, result1[2][i], sm2,
                                                   result1[4])
    return imine_count, imine_features_cond_1, imine_features_cond_2


imine_list=[]
for i in range(len(df_smiles)):
    if i == 78:
        continue
    val, cond1, cond2 = get_imine_condition(df_smiles[i])
    condition_1.append(cond1)
    condition_2.append(cond2)
    array_160_1 = np.zeros((NUM_ATOMS, 1))
    imine_list.append(array_160_1+val)

condition_1_array = np.array(condition_1, 'float32')
condition_2_array = np.array(condition_2, 'float32')
imine_features_array=np.array(imine_list,'float32')

print(get_imine_condition(
    ['CC(C)(c2ccc(OCC1CO1)cc2)c4ccc(OCC3OC3)cc4', 'CC(C)(c3ccc2OCN(c1ccccc1)Cc2c3)c6ccc5OCN(c4ccccc4)Cc5c6']))


class RelationalGraphConvLayer(keras.layers.Layer):
    def __init__(self, units=128, activation='relu', use_bias=False, kernel_initializer="glorot_uniform",
                 bias_initializer="zeros",
                 kernel_regularizer=None,
                 bias_regularizer=None, **kwargs):
        super().__init__(**kwargs)
        self.units = units
        self.activation = keras.activations.get(activation)
        self.use_bias = use_bias
        self.kernel_initializer = keras.initializers.get(kernel_initializer)
        self.bias_initializer = keras.initializers.get(bias_initializer)
        self.kernel_regularizer = keras.regularizers.get(kernel_regularizer)
        self.bias_regularizer = keras.regularizers.get(bias_regularizer)

    def build(self, input_shape):
        bond_dim = input_shape[0][1]
        atom_dim = input_shape[1][2]
        atom_dim = 172 +1
        self.kernel = self.add_weight(shape=(atom_dim, self.units),
                                      initializer=self.kernel_initializer,
                                      regularizer=self.kernel_regularizer,
                                      trainable=True,
                                      name="W",
                                      dtype=tf.float32,
                                      )

        if self.use_bias:
            self.bias = self.add_weight(shape=(1, self.units),
                                        initializer=self.bias_initializer,
                                        regularizer=self.bias_regularizer,
                                        trainable=True,
                                        name="b",
                                        dtype=tf.float32)
            self.built = True

    def call(self, inputs, training=False):
        adjacency_0, features_0, adjacency_1, features_1, imine_feature, conditional_feature_1, conditional_feature_2 = inputs

        x1 = keras.layers.Concatenate(axis=-1)([adjacency_0, features_0])  # tf.matmul(adjacency_0,features_0)
        conditional_features_transposed = tf.transpose(conditional_feature_1, (0, 2, 1))
        x=x1 = keras.layers.Concatenate(axis=-1)([x1,conditional_features_transposed]) #tf.add(x1,conditional_features_transposed)#
        x=tf.matmul(x1,self.kernel)

        x2 = keras.layers.Concatenate(axis=-1)([adjacency_1, features_1])  # tf.matmul(adjacency_0,features_0)
        conditional_features_transposed_2 = tf.transpose(conditional_feature_2, (0, 2, 1))
        y=x2 =  keras.layers.Concatenate(axis=-1)([x2,conditional_features_transposed_2])#tf.add(x2,conditional_features_transposed_2)#
        y= tf.matmul(x2, self.kernel)
        
        z=keras.layers.Concatenate(axis=-1)([x,y,imine_feature])

        if self.use_bias:
            x += self.bias
            y += self.bias
        return self.activation(z), self.activation(z)


def get_encoder(gconv_units, latent_dim, adjacency_shape, feature_shape, imine_shape, condition_shape,
                dense_units, dropout_rate):
    adjacency_0 = keras.layers.Input(shape=adjacency_shape)
    features_0 = keras.layers.Input(shape=feature_shape)
    adjacency_1 = keras.layers.Input(shape=adjacency_shape)
    features_1 = keras.layers.Input(shape=feature_shape)
    imine_feature = keras.layers.Input(shape=imine_shape)
    conditional_features_1 = keras.layers.Input(shape=condition_shape)
    conditional_features_2 = keras.layers.Input(shape=condition_shape)

    features_transformed_0 = features_0
    features_transformed_1 = features_1

    for units in gconv_units:
        features_transformed_0, features_transformed_1 = RelationalGraphConvLayer(units)(
            [adjacency_0, features_transformed_0, adjacency_1, features_transformed_1, imine_feature
                , conditional_features_1, conditional_features_2])

    x = keras.layers.GlobalAveragePooling1D()(features_transformed_0)
    y = keras.layers.GlobalAveragePooling1D()(features_transformed_1)
    for units in dense_units:
        y=x+y
        x = layers.Dense(units, activation='relu')(x)
        x = layers.Dropout(dropout_rate)(x)
        y = layers.Dense(units, activation='relu')(y)
        y = layers.Dropout(dropout_rate)(y)

    z_mean = layers.Dense(latent_dim, dtype='float32', name='z_mean')(x)
    log_var = layers.Dense(latent_dim, dtype='float32', name='log_var')(x)
    z_mean_1 = layers.Dense(latent_dim, dtype='float32', name='z_mean_1')(y)
    log_var_1 = layers.Dense(latent_dim, dtype='float32', name='log_var_1')(y)
    encoder = keras.Model([adjacency_0, features_0, adjacency_1, features_1,
                           imine_feature, conditional_features_1, conditional_features_2],
                          [z_mean, log_var, z_mean_1, log_var_1], name='encoder')

    return encoder


def get_decoder(dense_units, dropout_rate, latent_dim, adjacency_shape, feature_shape, condition_shape):
    latent_inputs = keras.Input(shape=(latent_dim,))
    latent_inputs_1 = keras.Input(shape=(latent_dim,))

    x = latent_inputs
    y = latent_inputs_1
    for units in dense_units:
        x = keras.layers.Dense(units, activation="tanh")(x)
        x = keras.layers.Dropout(dropout_rate)(x)
        y = keras.layers.Dense(units, activation="tanh")(y)
        y = keras.layers.Dropout(dropout_rate)(y)

    x_0_adjacency = keras.layers.Dense(tf.math.reduce_prod(adjacency_shape))(x)
    x_0_adjacency = keras.layers.Reshape(adjacency_shape)(x_0_adjacency)
    # Symmetrify tensors in the last two dimensions
    x_0_adjacency = (x_0_adjacency + tf.transpose(x_0_adjacency, (0, 2, 1)))

    # Map outputs of previous layer (x) to [continuous] feature tensors (x_features)
    x_0_features = keras.layers.Dense(tf.math.reduce_prod(feature_shape))(x)
    x_0_features = keras.layers.Reshape(feature_shape)(x_0_features)
    x_0_features = keras.layers.Softmax(axis=2)(x_0_features)

    # Map outputs of previous layer (x) to [continuous] adjacency tensors (x_adjacency)
    x_1_adjacency = keras.layers.Dense(tf.math.reduce_prod(adjacency_shape))(y)
    x_1_adjacency = keras.layers.Reshape(adjacency_shape)(x_1_adjacency)
    # Symmetrify tensors in the last two dimensions
    x_1_adjacency = (x_1_adjacency + tf.transpose(x_1_adjacency, (0, 2, 1)))

    # Map outputs of previous layer (x) to [continuous] feature tensors (x_features)
    x_1_features = keras.layers.Dense(tf.math.reduce_prod(feature_shape))(y)
    x_1_features = keras.layers.Reshape(feature_shape)(x_1_features)
    x_1_features = keras.layers.Softmax(axis=2)(x_1_features)

    decoder = keras.Model(
        [latent_inputs, latent_inputs_1], outputs=[x_0_adjacency, x_0_features, x_1_adjacency, x_1_features],
        name="decoder"
    )

    return decoder


class Sampling(layers.Layer):
    def call(self, inputs):
        z_mean, z_log_var, z_mean_1, z_log_var_1 = inputs
        batch = tf.shape(z_log_var)[0]
        dim = tf.shape(z_log_var)[1]
        epsilon = tf.keras.backend.random_normal(shape=(batch, dim))
        return z_mean + tf.exp(0.5 * z_log_var) * epsilon, z_mean_1 + tf.exp(0.5 * z_log_var_1) * epsilon


class MoleculeGenerator(keras.Model):
    def __init__(self, encoder, decoder, max_len, **kwargs):
        super().__init__(**kwargs)
        self.encoder = encoder
        self.decoder = decoder
        self.imine_prediction_layer = layers.Dense(1)
        self.max_len = max_len

        self.train_total_loss_tracker = keras.metrics.Mean(name="train_total_loss")
        self.train_loss1_tracker = keras.metrics.Mean(name="train_total_loss1")
        self.train_loss2_tracker = keras.metrics.Mean(name="train_total_loss2")
        self.val_total_loss_tracker = keras.metrics.Mean(name="val_total_loss")

    def _gradient_penalty(self, graph_real, graph_generated):
        # Unpack graphs
        adjacency_0_real, features_0_real, adjacency_1_real, features_1_real = graph_real
        adjacency_0_generated, features_0_generated, adjacency_1_generated, features_1_generated = graph_generated

        # Generate interpolated graphs (adjacency_interp and features_interp)
        alpha = tf.random.uniform([32])
        alpha = tf.reshape(alpha, (32, 1, 1))
        adjacency_0_interp = (adjacency_0_real * alpha) + (1 - alpha) * adjacency_0_generated
        adjacency_1_interp = (adjacency_1_real * alpha) + (1 - alpha) * adjacency_1_generated
        alpha = tf.reshape(alpha, (32, 1, 1))
        features_0_interp = (features_0_real * alpha) + (1 - alpha) * features_0_generated
        features_1_interp = (features_1_real * alpha) + (1 - alpha) * features_1_generated

        # Compute the logits of interpolated graphs
        with tf.GradientTape() as tape:
            tape.watch(adjacency_0_interp)
            tape.watch(features_0_interp)
            tape.watch(adjacency_1_interp)
            tape.watch(features_1_interp)
            _, _, _, _, _, _, _, _, logits = self(
                [adjacency_0_interp, features_0_interp, adjacency_1_interp, features_1_interp], training=True
            )

        # Compute the gradients with respect to the interpolated graphs
        grads = tape.gradient(logits, [adjacency_0_interp, features_0_interp, adjacency_1_interp, features_1_interp])

        # Compute the gradient penalty
        grads_adjacency_0_penalty = (1 - tf.norm(grads[0], axis=1)) ** 2
        grads_features_0_penalty = (1 - tf.norm(grads[1], axis=2)) ** 2
        grads_adjacency_1_penalty = (1 - tf.norm(grads[2], axis=1)) ** 2
        grads_features_1_penalty = (1 - tf.norm(grads[3], axis=2)) ** 2
        return tf.reduce_mean(
            tf.reduce_mean(grads_adjacency_0_penalty, axis=(-2, -1))
            + tf.reduce_mean(grads_features_0_penalty, axis=(-1))
            + tf.reduce_mean(grads_adjacency_1_penalty, axis=(-2, -1))  # need to check
            + tf.reduce_mean(grads_features_1_penalty, axis=(-1))  # need to check
        )

    def train_step(self, data):
        adjacency_0_tensor, feature_0_tensor, adjacency_1_tensor, feature_1_tensor, imine_tensor, condition_1, condition_2 = \
        data[0]
        graph_real = [adjacency_0_tensor, feature_0_tensor, adjacency_1_tensor, feature_1_tensor]
        # self.batch_size = tf.shape(adjacency_0_tensor)[0]

        with tf.GradientTape() as tape:
            z_mean, z_log_var, z_mean_1, z_log_var_1, gen_0_adjacency, gen_0_features, gen_1_adjacency, gen_1_features, imine_pred = self(
                [adjacency_0_tensor, feature_0_tensor, adjacency_1_tensor, feature_1_tensor, imine_tensor, condition_1,
                 condition_2], training=True)
            graph_generated = [gen_0_adjacency, gen_0_features, gen_1_adjacency, gen_1_features]
            total_loss = self._compute_loss(
                z_log_var, z_mean, z_mean_1, z_log_var_1, graph_real, graph_generated, imine_tensor, imine_pred
            )
        grads = tape.gradient([total_loss], self.trainable_weights)
        self.optimizer.apply_gradients(zip(grads, self.trainable_weights))

        self.train_total_loss_tracker.update_state(total_loss)
        # self.train_loss1_tracker.update_state(loss1)
        # self.train_loss2_tracker.update_state(loss2)
        return {'loss': self.train_total_loss_tracker.result()}
        # {'loss' : self.train_total_loss_tracker.result(),'loss1' : self.train_loss1_tracker.result(),'loss2' : self.train_loss2_tracker.result()}

    def cosine_similarity_loss(self, y_true, y_pred):
        dot_product = tf.reduce_sum(y_true * y_pred)
        norm_tensor1 = tf.norm(y_true)
        norm_tensor2 = tf.norm(y_pred)
        cosine_similarity = dot_product / (norm_tensor1 * norm_tensor2)
        return cosine_similarity

    def _compute_loss(self, z_log_var, z_mean, z_mean_1, z_log_var_1, graph_real, graph_generated, imine_tensor,
                      imine_pred):
        alpha = 0.01
        adjacency_0_real, features_0_real, adjacency_1_real, features_1_real = graph_real
        adjacency_0_gen, features_0_gen, adjacency_1_gen, features_1_gen = graph_generated

        adjacency_0_loss = tf.reduce_mean(
            tf.keras.losses.mean_squared_error(adjacency_0_real, adjacency_0_gen)
        )
        # adjacency_0_loss = tf.reduce_mean(
        #     tf.reduce_sum(
        #             keras.losses.categorical_crossentropy(adjacency_0_real, adjacency_0_gen),
        #             axis=(1),
        #         )
        #     )
        adjacency_1_loss = tf.reduce_mean(
            tf.keras.losses.mean_squared_error(adjacency_1_real, adjacency_1_gen)
        )
        # adj_0_similarity= self.cosine_similarity_loss(adjacency_0_real,adjacency_0_gen)
        # adjacency_0_loss = adjacency_0_loss + alpha*adj_0_similarity
        # adjacency_1_loss = tf.reduce_mean(
        #     tf.reduce_sum(
        #             keras.losses.categorical_crossentropy(adjacency_1_real, adjacency_1_gen),
        #             axis=(1),
        #         )
        #     )

        # adj_1_similarity= self.cosine_similarity_loss(adjacency_1_real,adjacency_1_gen)
        # adjacency_1_loss = adjacency_1_loss + alpha*adj_1_similarity
        features_0_loss = tf.reduce_mean(
            tf.reduce_sum(
                keras.losses.categorical_crossentropy(features_0_real, features_0_gen),
                axis=(1),
            )
        )
        # feat_0_similarity= self.cosine_similarity_loss(features_0_real,features_0_gen)
        # features_0_loss = features_0_loss + alpha*feat_0_similarity
        features_1_loss = tf.reduce_mean(
            tf.reduce_sum(
                keras.losses.categorical_crossentropy(features_1_real, features_1_gen),
                axis=(1),
            )
        )
        
        #weighted_loss = features_0_real * 1.0 * features_0_loss + (features_0_gen - features_0_real) * 1.0 * features_0_loss
        #mean_loss_1 = tf.reduce_mean(weighted_loss)
        
        #weighted_loss = features_1_real * 1.0 * features_1_loss + (features_1_gen - features_1_real) * 1.0 * features_1_loss
        #mean_loss_2 = tf.reduce_mean(weighted_loss)
        # feat_1_similarity= self.cosine_similarity_loss(features_1_real,features_1_gen)
        # features_1_loss = features_1_loss + alpha*feat_1_similarity

        kl_loss_0 = -0.5 * tf.reduce_sum(
            1 + z_log_var - tf.square(z_mean) - tf.exp(z_log_var), 1
        )
        kl_loss_0 = tf.reduce_mean(kl_loss_0)
        kl_loss_1 = -0.5 * tf.reduce_sum(
            1 + z_log_var_1 - tf.square(z_mean_1) - tf.exp(z_log_var_1), 1
        )
        kl_loss_1 = tf.reduce_mean(kl_loss_1)

        mse = keras.losses.MeanSquaredError(reduction=tf.keras.losses.Reduction.NONE)
        property_loss = 0  # mse(imine_tensor, imine_pred)
        # graph_loss = self._gradient_penalty(graph_real, graph_generated)
        # total_loss = K.mean(adjacency_0_loss + features_0_loss+kl_loss_0) +K.mean(adjacency_1_loss + features_1_loss+kl_loss_1)+ property_loss+graph_loss

        # if checkFunctionalGroups([[adjacency_0_gen], [features_0_gen],[adjacency_1_gen], [features_1_gen]]):
        #     total_loss= total_loss ** 0.5
        return adjacency_0_loss + features_1_loss + kl_loss_0 + adjacency_1_loss + features_0_loss + kl_loss_1 + property_loss + 0

        # return property_loss,adjacency_0_loss + features_0_loss+kl_loss_0, adjacency_1_loss + features_1_loss+kl_loss_1,graph_los

    def inference(self, batch_size, sample):
        #np.random.seed(121)
        #tf.random.set_seed(42)
        ad_0, feat_0, ad_1, feat_1 = smiles_to_graph_d(sample)
        g = [[ad_0], [feat_0], [ad_1], [feat_1]]
        all_mols = []
        imine_list=[]

        # temp1 = round(np.random.uniform(0, 4),3)
        # feat_0 = feat_0 + temp1
        # feat_1 = feat_1 + temp1
        random_imine_value = np.random.uniform(0, 2)  # random.randint(0, 1)
        array_160_1 = np.zeros((NUM_ATOMS, 1))
        imine_list.append(array_160_1+random_imine_value)
        
        _,cond1, cond2 = get_imine_condition(sample)
        print(random_imine_value)
        z_mean, log_var, z_mean_1, log_var_1 = model.encoder([np.array([ad_0]), np.array([feat_0]),
                                                              np.array([ad_1]), np.array([feat_1]),
                                                              np.array(imine_list),
                                                              np.array([cond1]), np.array([cond2])])
        z1, z2 = Sampling()([z_mean, log_var, z_mean_1, log_var_1])

        for i in range(batch_size):
            z = tf.random.normal((1, LATENT_DIM))
            z_p = tf.random.normal((1, LATENT_DIM))
            z3 = z1 + z  # np.multiply(z1, z) #z1+z#
            z4 = z2 + z_p  # np.multiply(z1, z) #z1+z#
            reconstruction_adjacnency_0, recontstruction_features_0, reconstruction_adjacnency_1, recontstruction_features_1 = model.decoder.predict(
                [z3, z4])

            adjacency_0 = tf.linalg.set_diag(reconstruction_adjacnency_0,
                                             tf.zeros(tf.shape(reconstruction_adjacnency_0)[:-1]))
            adjacency_0 = abs(reconstruction_adjacnency_0[0].astype(int))
            features_0 = tf.argmax(recontstruction_features_0, axis=2)
            features_0 = tf.one_hot(features_0, depth=ATOM_DIM, axis=2)

            adjacency_1 = tf.linalg.set_diag(reconstruction_adjacnency_1,
                                             tf.zeros(tf.shape(reconstruction_adjacnency_1)[:-1]))
            features_1 = tf.argmax(recontstruction_features_1, axis=2)
            features_1 = tf.one_hot(features_1, depth=ATOM_DIM, axis=2)
            adjacency_1 = abs(reconstruction_adjacnency_1[0].astype(int))
            # graph2=[[adjacency_0[0].numpy()],[features_0[0].numpy()],[adjacency_1[0].numpy()],[features_1[0].numpy()]]
            graph2 = [[adjacency_0], [features_0[0].numpy()],
                      [adjacency_1], [features_0[0].numpy()]]
            all_mols.append(graph_to_molecule2_d(graph2))
        return all_mols

    def call(self, inputs):
        z_mean, log_var, z_mean_1, log_var_1 = self.encoder(inputs)
        z, z1 = Sampling()([z_mean, log_var, z_mean_1, log_var_1])
        gen_adjacency_0, gen_features_0, gen_adjacency_1, gen_features_1 = self.decoder([z, z1])
        imine_pred = self.imine_prediction_layer(z_mean + z_mean_1)

        return z_mean, log_var, z_mean_1, log_var_1, gen_adjacency_0, gen_features_0, gen_adjacency_1, gen_features_1, imine_pred


vae_optimizer = tf.keras.optimizers.Adam(learning_rate=VAE_LR)

encoder = get_encoder(
    gconv_units=[9],
    adjacency_shape=(NUM_ATOMS, NUM_ATOMS),
    feature_shape=(NUM_ATOMS, ATOM_DIM),
    imine_shape= (NUM_ATOMS,1),
    condition_shape=(1,NUM_ATOMS),
    latent_dim=LATENT_DIM,
    dense_units=[128,256, 512],
    dropout_rate=0.2,
)
decoder = get_decoder(
    dense_units=[128,256, 512, 1024],
    dropout_rate=0.2,
    latent_dim=LATENT_DIM,
    adjacency_shape=(NUM_ATOMS, NUM_ATOMS),
    feature_shape=(NUM_ATOMS, ATOM_DIM),
    condition_shape=(1,NUM_ATOMS)
)

model = MoleculeGenerator(encoder, decoder, 160)

checkpoint_path = "/home/C00521897/Fall 22/New_Monomer_generation/checkpoints1/cp-{epoch:04d}.ckpt"
checkpoint_dir = os.path.dirname(checkpoint_path)
n_batches = len(adjacency_0_tensor) / BATCH_SIZE
n_batches = math.ceil(n_batches)
cp_callback = tf.keras.callbacks.ModelCheckpoint(
    filepath=checkpoint_path,
    verbose=1,
    save_weights_only=True,
    save_freq=5 * n_batches)

stopping_callback = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=15, verbose=1, restore_best_weights=True)
# model.save_weights(checkpoint_path.format(epoch=0))

model.compile(vae_optimizer)
history = model.fit([adjacency_0_tensor, feature_0_tensor, adjacency_1_tensor, feature_1_tensor,
                     imine_features_array, condition_1_array, condition_2_array],
                    epochs=100,callbacks=[stopping_callback])


def write_samples_in_file():
    mols = []
    i=0
    with open('/home/C00521897/Fall 22/New_Monomer_generation/Data/output/output_10065.txt', 'a') as the_file:
        # random.seed(121)
        smiles =random.choices(df_smiles, k=20)
        for sample in smiles:
            i = i+1
            text = '\n-----------------'+str(i)+'---------------------------------\n'
            mols = model.inference(5, sample)
            the_file.write(text)
            the_file.write(str(sample))
            the_file.write('\n--------------------------------------------------\n')
            for index, m in enumerate(mols):
                if m[0][0] is not None:
                    smiles = Chem.MolToSmiles(m[0][0])
                else:
                    smiles = "None"
                smiles = smiles + ","
                if m[0][1] is not None:
                    smiles += Chem.MolToSmiles(m[0][1]) + "\n"
                else:
                    smiles += "None\n"
                the_file.write(smiles)


write_samples_in_file()


