{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1525764-dd62-49cc-9786-785f330eac19",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, AdamW\n",
    "import torch\n",
    "\n",
    "# Load the tokenizer and model\n",
    "\n",
    "model_name_or_id = \"AI4Chem/CHEMLLM-2b-1_5\"  # openai-community/gpt2-large\", \"AI4Chem/ChemLLM-20B-Chat-SFT\"\n",
    "\n",
    "# Check if CUDA is available and set the device\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Load model and tokenizer\n",
    "model = AutoModelForCausalLM.from_pretrained(f\"model/{model_name_or_id}\", torch_dtype=torch.float16, trust_remote_code=True).to(device)\n",
    "tokenizer = AutoTokenizer.from_pretrained(f\"tokenizer/{model_name_or_id}\", trust_remote_code=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token \n",
    "\n",
    "\n",
    "# Function to concatenate user and assistant messages into a single text string\n",
    "def concatenate_messages(dataset):\n",
    "    conversation = \"\"\n",
    "    for message in dataset[\"messages\"]:\n",
    "        if message[\"role\"] == \"user\":\n",
    "            conversation += \"User: \" + message[\"content\"] + \"\\n\"\n",
    "        elif message[\"role\"] == \"assistant\":\n",
    "            conversation += \"Assistant: \" + message[\"content\"] + \"\\n\"\n",
    "    return conversation\n",
    "\n",
    "# Prepare the texts by concatenating messages\n",
    "texts = [concatenate_messages(dataset) for dataset in datasets]\n",
    "\n",
    "# Tokenize the concatenated texts\n",
    "tokenized_inputs = tokenizer(texts, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "\n",
    "# Move the model and inputs to the correct device (GPU or CPU)\n",
    "input_ids = tokenized_inputs.input_ids.to(device)\n",
    "attention_mask = tokenized_inputs.attention_mask.to(device)\n",
    "\n",
    "# Define the optimizer\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
    "\n",
    "# Fine-tuning loop\n",
    "num_epochs = 3  # Number of epochs for training\n",
    "model.train()\n",
    "for epoch in range(num_epochs):\n",
    "    optimizer.zero_grad()\n",
    "    outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=input_ids)\n",
    "    loss = outputs.loss\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {loss.item()}\")\n",
    "\n",
    "# Save the fine-tuned model\n",
    "model.save_pretrained(\"fine_tuned_model\")\n",
    "tokenizer.save_pretrained(\"fine_tuned_model\")\n",
    "\n",
    "# Generate a response using the fine-tuned model\n",
    "model.eval()\n",
    "\n",
    "# Prepare an example input\n",
    "example_input = \"User: Can you suggest a polymer with Tg=300 and Er=200?\\nAssistant:\"\n",
    "\n",
    "# Tokenize the example input\n",
    "example_input_ids = tokenizer(example_input, return_tensors=\"pt\").input_ids.to(device)\n",
    "\n",
    "# Generate a response\n",
    "generated_ids = model.generate(example_input_ids, max_new_tokens=50, pad_token_id=tokenizer.eos_token_id)\n",
    "\n",
    "# Decode and print the response\n",
    "generated_text = tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n",
    "print(\"Generated Response:\")\n",
    "print(generated_text)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
