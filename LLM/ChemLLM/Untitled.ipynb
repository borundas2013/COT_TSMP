{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "663cc268-39ac-423f-8aab-d502961b553b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "570e842a-ce60-4e50-84e6-327d636df234",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Explicitly passing a `revision` is encouraged when loading a configuration with custom code to ensure no malicious code has been contributed in a newer revision.\n",
      "Explicitly passing a `revision` is encouraged when loading a model with custom code to ensure no malicious code has been contributed in a newer revision.\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "This modeling file requires the following packages that were not found in your environment: flash_attn, einops. Run `pip install flash_attn einops`",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_8124\\1504231051.py\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mmodel_name_or_id\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"AI4Chem/ChemLLM-20B-Chat-SFT\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mAutoModelForCausalLM\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel_name_or_id\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtorch_dtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat16\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdevice_map\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"auto\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtrust_remote_code\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mtokenizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mAutoTokenizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel_name_or_id\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtrust_remote_code\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\models\\auto\\auto_factory.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[0;32m    453\u001b[0m             \u001b[0mclass_ref\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconfig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mauto_map\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcls\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    454\u001b[0m             \u001b[0mmodule_file\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclass_name\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mclass_ref\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\".\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 455\u001b[1;33m             model_class = get_class_from_dynamic_module(\n\u001b[0m\u001b[0;32m    456\u001b[0m                 \u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodule_file\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m\".py\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclass_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mhub_kwargs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    457\u001b[0m             )\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\dynamic_module_utils.py\u001b[0m in \u001b[0;36mget_class_from_dynamic_module\u001b[1;34m(pretrained_model_name_or_path, module_file, class_name, cache_dir, force_download, resume_download, proxies, use_auth_token, revision, local_files_only, **kwargs)\u001b[0m\n\u001b[0;32m    361\u001b[0m     ```\"\"\"\n\u001b[0;32m    362\u001b[0m     \u001b[1;31m# And lastly we get the class inside our newly created module\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 363\u001b[1;33m     final_module = get_cached_module_file(\n\u001b[0m\u001b[0;32m    364\u001b[0m         \u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    365\u001b[0m         \u001b[0mmodule_file\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\dynamic_module_utils.py\u001b[0m in \u001b[0;36mget_cached_module_file\u001b[1;34m(pretrained_model_name_or_path, module_file, cache_dir, force_download, resume_download, proxies, use_auth_token, revision, local_files_only)\u001b[0m\n\u001b[0;32m    235\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    236\u001b[0m     \u001b[1;31m# Check we have all the requirements in our environment\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 237\u001b[1;33m     \u001b[0mmodules_needed\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck_imports\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresolved_module_file\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    238\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    239\u001b[0m     \u001b[1;31m# Now we move the module inside our cached dynamic modules.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\dynamic_module_utils.py\u001b[0m in \u001b[0;36mcheck_imports\u001b[1;34m(filename)\u001b[0m\n\u001b[0;32m    132\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    133\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmissing_packages\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 134\u001b[1;33m         raise ImportError(\n\u001b[0m\u001b[0;32m    135\u001b[0m             \u001b[1;34m\"This modeling file requires the following packages that were not found in your environment: \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    136\u001b[0m             \u001b[1;34mf\"{', '.join(missing_packages)}. Run `pip install {' '.join(missing_packages)}`\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mImportError\u001b[0m: This modeling file requires the following packages that were not found in your environment: flash_attn, einops. Run `pip install flash_attn einops`"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, GenerationConfig\n",
    "import torch\n",
    "\n",
    "model_name_or_id = \"AI4Chem/CHEMLLM-2b-1_5\"  # openai-community/gpt2-large\", \"AI4Chem/ChemLLM-20B-Chat-SFT\"\n",
    "\n",
    "# Check if CUDA is available and set the device\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Load model and tokenizer\n",
    "model = AutoModelForCausalLM.from_pretrained(f\"model/{model_name_or_id}\", torch_dtype=torch.float16, trust_remote_code=True).to(device)\n",
    "tokenizer = AutoTokenizer.from_pretrained(f\"tokenizer/{model_name_or_id}\", trust_remote_code=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token \n",
    "\n",
    "# Prepare input\n",
    "#prompt = \"What is the Molecule of Ibuprofen?\"\n",
    "# prompt ='Who are you?'\n",
    "# inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "\n",
    "# # Set generation configuration\n",
    "# generation_config = GenerationConfig(\n",
    "#     do_sample=True,\n",
    "#     top_k=1,\n",
    "#     temperature=0.9,\n",
    "#     max_new_tokens=500,\n",
    "#     repetition_penalty=1.5,\n",
    "#     pad_token_id=tokenizer.eos_token_id\n",
    "# )\n",
    "\n",
    "# # Generate text\n",
    "# outputs = model.generate(**inputs, generation_config=generation_config)\n",
    "\n",
    "# # Decode and print the output\n",
    "# print(tokenizer.decode(outputs[0], skip_special_tokens=True))\n",
    "\n",
    "\n",
    "# from datasets import load_dataset\n",
    "\n",
    "# ds = load_dataset(\"AI4Chem/ChemBench4K\")\n",
    "\n",
    "# from datasets import load_dataset\n",
    "# data_path=\"fine_tune_data.json\"\n",
    "# dataset=load_dataset('json', data_files=data_path)\n",
    "\n",
    "def tokenize_function(examples):\n",
    "\n",
    "  inputs=examples['prompt']\n",
    "  targets=examples['responses']\n",
    "  tokenizer.pad_token = tokenizer.eos_token\n",
    "  model_inputs=tokenizer(inputs, max_length=512,truncation=True, padding='max_length')\n",
    "  print(model_inputs)\n",
    "  labels = tokenizer(targets, max_length=512, truncation=True, padding=\"max_length\")\n",
    "  model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "  return model_inputs\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)\n",
    "print(tokenized_datasets.data)\n",
    "\n",
    "# from torch.utils.data import DataLoader\n",
    "# train_datasets=tokenized_datasets['train']\n",
    "# train_dataloader = DataLoader(train_datasets, shuffle=True)\n",
    "\n",
    "# train_dataloader\n",
    "\n",
    "# from transformers import AutoModelForCausalLM, Trainer, TrainingArguments\n",
    "\n",
    "\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"/ddnB/work/borun22/llm/results\",\n",
    "    learning_rate=1e-5,  # Lowered learning rate\n",
    "    per_device_train_batch_size=1,\n",
    "    per_device_eval_batch_size=1,\n",
    "    num_train_epochs=100,\n",
    "    weight_decay=0.01, # Use mixed precision training if supported\n",
    "    logging_dir='/ddnB/work/borun22/llm/logs',\n",
    "    logging_steps=50,\n",
    "    fp16=False,\n",
    "    gradient_accumulation_steps=8,  # Use gradient accumulation\n",
    "    max_grad_norm=1.0\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_datasets,\n",
    "    eval_dataset=train_datasets,  # Use validation set if available\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "# import torch\n",
    "\n",
    "# # Check if CUDA (GPU) is available\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# # Move the model to the GPU if available\n",
    "# model.to(device)\n",
    "\n",
    "# # Prepare the input text\n",
    "# query = \"can you sugggest  a TSMP with Tg=350 and Er=150.\"\n",
    "\n",
    "# # Tokenize the input text with padding\n",
    "# inputs = tokenizer(query, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "# print(inputs)\n",
    "\n",
    "# # Move input tensors to the same device as the model\n",
    "# inputs = {key: value.to(device) for key, value in inputs.items()}\n",
    "\n",
    "# # Generate the response\n",
    "# outputs = model.generate(inputs['input_ids'], attention_mask=inputs['attention_mask'], max_length=100)\n",
    "\n",
    "# # Decode the generated tokens to text\n",
    "# response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "# print(f\"Prompt: {query}\\nResponse: {response}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, GenerationConfig\n",
    "import torch\n",
    "\n",
    "model_name_or_id = \"AI4Chem/CHEMLLM-2b-1_5\"  # openai-community/gpt2-large\", \"AI4Chem/ChemLLM-20B-Chat-SFT\"\n",
    "\n",
    "# Check if CUDA is available and set the device\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Load model and tokenizer\n",
    "model = AutoModelForCausalLM.from_pretrained(f\"model/{model_name_or_id}\", torch_dtype=torch.float16, trust_remote_code=True).to(device)\n",
    "tokenizer = AutoTokenizer.from_pretrained(f\"tokenizer/{model_name_or_id}\", trust_remote_code=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token \n",
    "\n",
    "# Prepare input\n",
    "#prompt = \"What is the Molecule of Ibuprofen?\"\n",
    "# prompt ='Who are you?'\n",
    "# inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "\n",
    "# # Set generation configuration\n",
    "# generation_config = GenerationConfig(\n",
    "#     do_sample=True,\n",
    "#     top_k=1,\n",
    "#     temperature=0.9,\n",
    "#     max_new_tokens=500,\n",
    "#     repetition_penalty=1.5,\n",
    "#     pad_token_id=tokenizer.eos_token_id\n",
    "# )\n",
    "\n",
    "# # Generate text\n",
    "# outputs = model.generate(**inputs, generation_config=generation_config)\n",
    "\n",
    "# # Decode and print the output\n",
    "# print(tokenizer.decode(outputs[0], skip_special_tokens=True))\n",
    "\n",
    "\n",
    "# from datasets import load_dataset\n",
    "\n",
    "# ds = load_dataset(\"AI4Chem/ChemBench4K\")\n",
    "\n",
    "from datasets import load_dataset\n",
    "data_path=\"fine_tune_data.json\"\n",
    "dataset=load_dataset('json', data_files=data_path)\n",
    "\n",
    "def tokenize_function(examples):\n",
    "\n",
    "  inputs=examples['prompt']\n",
    "  targets=examples['responses']\n",
    "  tokenizer.pad_token = tokenizer.eos_token\n",
    "  model_inputs=tokenizer(inputs, max_length=512,truncation=True, padding='max_length')\n",
    "  print(model_inputs)\n",
    "  labels = tokenizer(targets, max_length=512, truncation=True, padding=\"max_length\")\n",
    "  model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "  return model_inputs\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)\n",
    "print(tokenized_datasets.data)\n",
    "\n",
    "# from torch.utils.data import DataLoader\n",
    "train_datasets=tokenized_datasets['train']\n",
    "# train_dataloader = DataLoader(train_datasets, shuffle=True)\n",
    "\n",
    "# train_dataloader\n",
    "\n",
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"/ddnB/work/borun22/llm/results\",\n",
    "    learning_rate=1e-5,  # Lowered learning rate\n",
    "    per_device_train_batch_size=1,\n",
    "    per_device_eval_batch_size=1,\n",
    "    num_train_epochs=100,\n",
    "    weight_decay=0.01, # Use mixed precision training if supported\n",
    "    logging_dir='/ddnB/work/borun22/llm/logs',\n",
    "    logging_steps=50,\n",
    "    fp16=False,\n",
    "    gradient_accumulation_steps=8,  # Use gradient accumulation\n",
    "    max_grad_norm=1.0\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_datasets,\n",
    "    eval_dataset=train_datasets,  # Use validation set if available\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "# import torch\n",
    "\n",
    "# # Check if CUDA (GPU) is available\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# # Move the model to the GPU if available\n",
    "# model.to(device)\n",
    "\n",
    "# # Prepare the input text\n",
    "query = \"can you sugggest  a TSMP with Tg=350 and Er=150.\"\n",
    "\n",
    "# # Tokenize the input text with padding\n",
    "inputs = tokenizer(query, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "print(inputs)\n",
    "\n",
    "# # Move input tensors to the same device as the model\n",
    "inputs = {key: value.to(device) for key, value in inputs.items()}\n",
    "\n",
    "# # Generate the response\n",
    "outputs = model.generate(inputs['input_ids'], attention_mask=inputs['attention_mask'], max_length=100)\n",
    "\n",
    "# # Decode the generated tokens to text\n",
    "response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "print(f\"Prompt: {query}\\nResponse: {response}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0749dc25-7859-4914-b74f-8f7b5b4ee2d8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0db8e9c0-1387-4aa7-9684-4e322b699569",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v12.6\n",
      "C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v12.6\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(os.environ.get('CUDA_PATH'))\n",
    "print(os.environ.get('CUDA_HOME'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3fe51393-0f05-4127-a924-264355ff23c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration AI4Chem--dpo-mix-7k-alpaca-en-5bdaaa66c389b4d0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset parquet/AI4Chem--dpo-mix-7k-alpaca-en to C:/Users/borun/.cache/huggingface/datasets/AI4Chem___parquet/AI4Chem--dpo-mix-7k-alpaca-en-5bdaaa66c389b4d0/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "45bfb7be38684ca8889ecc0bf3246dca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab07dcee267b442a9cb80d09c57992bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting data files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "362e7f66ecc546008bd362b326f15ffa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0 tables [00:00, ? tables/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "PermissionError",
     "evalue": "[WinError 32] The process cannot access the file because it is being used by another process: 'C:/Users/borun/.cache/huggingface/datasets/AI4Chem___parquet/AI4Chem--dpo-mix-7k-alpaca-en-5bdaaa66c389b4d0/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec.incomplete\\\\parquet-train.arrow'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\datasets\\builder.py\u001b[0m in \u001b[0;36m_prepare_split\u001b[1;34m(self, split_generator, file_format, max_shard_size)\u001b[0m\n\u001b[0;32m   1512\u001b[0m                     )\n\u001b[1;32m-> 1513\u001b[1;33m                 \u001b[0mwriter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite_table\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtable\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1514\u001b[0m         \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\datasets\\arrow_writer.py\u001b[0m in \u001b[0;36mwrite_table\u001b[1;34m(self, pa_table, writer_batch_size)\u001b[0m\n\u001b[0;32m    538\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpa_writer\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 539\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_build_writer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minferred_schema\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpa_table\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mschema\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    540\u001b[0m         \u001b[0mpa_table\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtable_cast\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpa_table\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_schema\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\datasets\\arrow_writer.py\u001b[0m in \u001b[0;36m_build_writer\u001b[1;34m(self, inferred_schema)\u001b[0m\n\u001b[0;32m    366\u001b[0m         \u001b[0mschema\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mschema\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 367\u001b[1;33m         \u001b[0minferred_features\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mFeatures\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_arrow_schema\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minferred_schema\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    368\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_features\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\datasets\\features\\features.py\u001b[0m in \u001b[0;36mfrom_arrow_schema\u001b[1;34m(cls, pa_schema)\u001b[0m\n\u001b[0;32m   1567\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;34m\"info\"\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mmetadata\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;34m\"features\"\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mmetadata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"info\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mmetadata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"info\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"features\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1568\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mFeatures\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_dict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmetadata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"info\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"features\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1569\u001b[0m         \u001b[0mobj\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[0mfield\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mgenerate_from_arrow_type\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfield\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mfield\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mpa_schema\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\datasets\\features\\features.py\u001b[0m in \u001b[0;36mfrom_dict\u001b[1;34m(cls, dic)\u001b[0m\n\u001b[0;32m   1596\u001b[0m         \"\"\"\n\u001b[1;32m-> 1597\u001b[1;33m         \u001b[0mobj\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgenerate_from_dict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdic\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1598\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\datasets\\features\\features.py\u001b[0m in \u001b[0;36mgenerate_from_dict\u001b[1;34m(obj)\u001b[0m\n\u001b[0;32m   1279\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;34m\"_type\"\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mobj\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"_type\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1280\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[1;33m{\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mgenerate_from_dict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mobj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1281\u001b[0m     \u001b[0mclass_type\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mglobals\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"_type\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\datasets\\features\\features.py\u001b[0m in \u001b[0;36m<dictcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m   1279\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;34m\"_type\"\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mobj\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"_type\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1280\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[1;33m{\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mgenerate_from_dict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mobj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1281\u001b[0m     \u001b[0mclass_type\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mglobals\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"_type\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\datasets\\features\\features.py\u001b[0m in \u001b[0;36mgenerate_from_dict\u001b[1;34m(obj)\u001b[0m\n\u001b[0;32m   1283\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mclass_type\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mSequence\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1284\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mSequence\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeature\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mgenerate_from_dict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"feature\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlength\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"length\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1285\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'length'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\datasets\\builder.py\u001b[0m in \u001b[0;36mincomplete_dir\u001b[1;34m(dirname)\u001b[0m\n\u001b[0;32m    757\u001b[0m                     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 758\u001b[1;33m                         \u001b[1;32myield\u001b[0m \u001b[0mtmp_dir\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    759\u001b[0m                         \u001b[1;32mif\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0misdir\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdirname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\datasets\\builder.py\u001b[0m in \u001b[0;36mdownload_and_prepare\u001b[1;34m(self, output_dir, download_config, download_mode, ignore_verifications, try_from_hf_gcs, dl_manager, base_path, use_auth_token, file_format, max_shard_size, storage_options, **download_and_prepare_kwargs)\u001b[0m\n\u001b[0;32m    806\u001b[0m                         }\n\u001b[1;32m--> 807\u001b[1;33m                         self._download_and_prepare(\n\u001b[0m\u001b[0;32m    808\u001b[0m                             \u001b[0mdl_manager\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdl_manager\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\datasets\\builder.py\u001b[0m in \u001b[0;36m_download_and_prepare\u001b[1;34m(self, dl_manager, verify_infos, **prepare_split_kwargs)\u001b[0m\n\u001b[0;32m    897\u001b[0m                 \u001b[1;31m# Prepare split will record examples associated to the split\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 898\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_prepare_split\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msplit_generator\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mprepare_split_kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    899\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\datasets\\builder.py\u001b[0m in \u001b[0;36m_prepare_split\u001b[1;34m(self, split_generator, file_format, max_shard_size)\u001b[0m\n\u001b[0;32m   1515\u001b[0m             \u001b[0mnum_shards\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mshard_id\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1516\u001b[1;33m             \u001b[0mnum_examples\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_bytes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mwriter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfinalize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1517\u001b[0m             \u001b[0mwriter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\datasets\\arrow_writer.py\u001b[0m in \u001b[0;36mfinalize\u001b[1;34m(self, close_stream)\u001b[0m\n\u001b[0;32m    558\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 559\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Please pass `features` or at least one example when writing data\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    560\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpa_writer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Please pass `features` or at least one example when writing data",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mPermissionError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_11840\\4074146732.py\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mdatasets\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mload_dataset\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mds\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mload_dataset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"AI4Chem/dpo-mix-7k-alpaca-en\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\datasets\\load.py\u001b[0m in \u001b[0;36mload_dataset\u001b[1;34m(path, name, data_dir, data_files, split, cache_dir, features, download_config, download_mode, ignore_verifications, keep_in_memory, save_infos, revision, use_auth_token, task, streaming, **config_kwargs)\u001b[0m\n\u001b[0;32m   1691\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1692\u001b[0m     \u001b[1;31m# Download and prepare data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1693\u001b[1;33m     builder_instance.download_and_prepare(\n\u001b[0m\u001b[0;32m   1694\u001b[0m         \u001b[0mdownload_config\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdownload_config\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1695\u001b[0m         \u001b[0mdownload_mode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdownload_mode\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\datasets\\builder.py\u001b[0m in \u001b[0;36mdownload_and_prepare\u001b[1;34m(self, output_dir, download_config, download_mode, ignore_verifications, try_from_hf_gcs, dl_manager, base_path, use_auth_token, file_format, max_shard_size, storage_options, **download_and_prepare_kwargs)\u001b[0m\n\u001b[0;32m    784\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    785\u001b[0m             \u001b[1;31m# Create a tmp dir and rename to self._output_dir on successful exit.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 786\u001b[1;33m             \u001b[1;32mwith\u001b[0m \u001b[0mincomplete_dir\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_output_dir\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtmp_output_dir\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    787\u001b[0m                 \u001b[1;31m# Temporarily assign _output_dir to tmp_data_dir to avoid having to forward\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    788\u001b[0m                 \u001b[1;31m# it to every sub function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\contextlib.py\u001b[0m in \u001b[0;36m__exit__\u001b[1;34m(self, typ, value, traceback)\u001b[0m\n\u001b[0;32m    151\u001b[0m                 \u001b[0mvalue\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtyp\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    152\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 153\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgen\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mthrow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtyp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtraceback\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    154\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    155\u001b[0m                 \u001b[1;31m# Suppress StopIteration *unless* it's the same exception that\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\datasets\\builder.py\u001b[0m in \u001b[0;36mincomplete_dir\u001b[1;34m(dirname)\u001b[0m\n\u001b[0;32m    763\u001b[0m                     \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    764\u001b[0m                         \u001b[1;32mif\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtmp_dir\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 765\u001b[1;33m                             \u001b[0mshutil\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrmtree\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtmp_dir\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    766\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    767\u001b[0m             \u001b[1;31m# Print is intentional: we want this to always go to stdout so user has\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\shutil.py\u001b[0m in \u001b[0;36mrmtree\u001b[1;34m(path, ignore_errors, onerror)\u001b[0m\n\u001b[0;32m    747\u001b[0m             \u001b[1;31m# can't continue even if onerror hook returns\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    748\u001b[0m             \u001b[1;32mreturn\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 749\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_rmtree_unsafe\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0monerror\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    750\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    751\u001b[0m \u001b[1;31m# Allow introspection of whether or not the hardening against symlink\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\shutil.py\u001b[0m in \u001b[0;36m_rmtree_unsafe\u001b[1;34m(path, onerror)\u001b[0m\n\u001b[0;32m    617\u001b[0m                 \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munlink\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfullname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    618\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 619\u001b[1;33m                 \u001b[0monerror\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munlink\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfullname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexc_info\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    620\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    621\u001b[0m         \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrmdir\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\shutil.py\u001b[0m in \u001b[0;36m_rmtree_unsafe\u001b[1;34m(path, onerror)\u001b[0m\n\u001b[0;32m    615\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    616\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 617\u001b[1;33m                 \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munlink\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfullname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    618\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    619\u001b[0m                 \u001b[0monerror\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munlink\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfullname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexc_info\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mPermissionError\u001b[0m: [WinError 32] The process cannot access the file because it is being used by another process: 'C:/Users/borun/.cache/huggingface/datasets/AI4Chem___parquet/AI4Chem--dpo-mix-7k-alpaca-en-5bdaaa66c389b4d0/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec.incomplete\\\\parquet-train.arrow'"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "ds = load_dataset(\"AI4Chem/dpo-mix-7k-alpaca-en\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "26778489-0eb7-4fb5-a1ce-80d02e5de864",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    test: Dataset({\n",
       "        features: ['question', 'B', 'answer', 'A', 'D', 'C'],\n",
       "        num_rows: 4009\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['question', 'B', 'answer', 'A', 'D', 'C'],\n",
       "        num_rows: 45\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "10cd9bfb-7e18-4966-a0fe-5cd4043bb0ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyarrow.StringScalar: 'Create a molecule that satisfies the conditions outlined in the description.\\nThe molecule is a natural product found in Juglans nigra with data available.'>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds.data['test']['question'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "6083aa34-7d7b-43a2-bae6-e16a7f8cfd55",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyarrow.StringScalar: 'My response is C(C)CCCC(S)(C)C'>"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds.data['test']['B'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23d56de2-8535-4860-8f4b-e013d94a53df",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
